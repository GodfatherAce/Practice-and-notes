{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Read data from csv\n",
    "pga = pandas.read_csv(\"pga.csv\")\n",
    "\n",
    "# Normalize the data\n",
    "pga.distance = (pga.distance - pga.distance.mean()) / pga.distance.std()\n",
    "pga.accuracy = (pga.accuracy - pga.accuracy.mean()) / pga.accuracy.std()\n",
    "print(pga.head())\n",
    "\n",
    "plt.scatter(pga.distance, pga.accuracy)\n",
    "plt.xlabel('normalized distance')\n",
    "plt.ylabel('normalized accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimate a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# We can add a dimension to an array by using np.newaxis\n",
    "print(\"Shape of the series:\", pga.distance.shape)\n",
    "print(\"Shape with newaxis:\", pga.distance[:, np.newaxis].shape)\n",
    "\n",
    "# The X variable in LinearRegression.fit() must have 2 dimensions\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(pga.distance[:,np.newaxis], pga.accuracy)\n",
    "print(lr.coef_.shape)\n",
    "theta1 = lr.coef_[0]\n",
    "\n",
    "print(theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The cost function of a single variable linear model\n",
    "def cost(theta0, theta1, x, y):\n",
    "    # Initialize cost\n",
    "    J = 0\n",
    "    # The number of observations\n",
    "    m = len(x)\n",
    "    # Loop through each observation\n",
    "    for i in range(m):\n",
    "        # Compute the hypothesis \n",
    "        h = theta1 * x[i] + theta0\n",
    "        # Add to cost\n",
    "        J += (h - y[i])**2\n",
    "    # Average and normalize cost\n",
    "    J /= (2*m)\n",
    "    return J\n",
    "\n",
    "# The cost for theta0=0 and theta1=1\n",
    "print(cost(0, 1, pga.distance, pga.accuracy))\n",
    "\n",
    "theta0 = 100\n",
    "theta1s = np.linspace(-3,2,100)\n",
    "costs = []\n",
    "for theta1 in theta1s:\n",
    "    costs.append(cost(theta0, theta1, pga.distance, pga.accuracy))\n",
    "\n",
    "plt.plot(theta1s, costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Example of a Surface Plot using Matplotlib\n",
    "# Create x an y variables\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.linspace(-10,10,100)\n",
    "\n",
    "# We must create variables to represent each possible pair of points in x and y\n",
    "# ie. (-10, 10), (-10, -9.8), ... (0, 0), ... ,(10, 9.8), (10,9.8)\n",
    "# x and y need to be transformed to 100x100 matrices to represent these coordinates\n",
    "# np.meshgrid will build a coordinate matrices of x and y\n",
    "X, Y = np.meshgrid(x,y)\n",
    "print(X[:5,:5],\"\\n\",Y[:5,:5])\n",
    "\n",
    "# Compute a 3D parabola \n",
    "Z = X**2 + Y**2 \n",
    "\n",
    "# Open a figure to place the plot on\n",
    "fig = plt.figure()\n",
    "# Initialize 3D plot\n",
    "ax = fig.gca(projection='3d')\n",
    "# Plot the surface\n",
    "ax.plot_surface(X=X,Y=Y,Z=Z)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Use these for your excerise \n",
    "theta0s = np.linspace(-2,2,100)\n",
    "theta1s = np.linspace(-2,2, 100)\n",
    "COST = np.empty(shape=(100,100))\n",
    "# Meshgrid for paramaters \n",
    "T0S, T1S = np.meshgrid(theta0s, theta1s)\n",
    "# for each parameter combination compute the cost\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        COST[i,j] = cost(T0S[0,i], T1S[j,0], pga.distance, pga.accuracy)\n",
    "\n",
    "# make 3d plot\n",
    "fig2 = plt.figure()\n",
    "ax = fig2.gca(projection='3d')\n",
    "ax.plot_surface(X=T0S,Y=T1S,Z=COST)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating the partial derivative of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_cost_theta1(theta0, theta1, x, y):\n",
    "    # Hypothesis\n",
    "    h = theta0 + theta1*x\n",
    "    # Hypothesis minus observed times x\n",
    "    diff = (h - y) * x\n",
    "    # Average to compute partial derivative\n",
    "    partial = diff.sum() / (x.shape[0])\n",
    "    return partial\n",
    "\n",
    "partial1 = partial_cost_theta1(0, 5, pga.distance, pga.accuracy)\n",
    "print(\"partial1 =\", partial1)\n",
    "# Partial derivative of cost in terms of theta0\n",
    "def partial_cost_theta0(theta0, theta1, x, y):\n",
    "    # Hypothesis\n",
    "    h = theta0 + theta1*x\n",
    "    # Difference between hypothesis and observation\n",
    "    diff = (h - y)\n",
    "    # Compute partial derivative\n",
    "    partial = diff.sum() / (x.shape[0])\n",
    "    return partial\n",
    "\n",
    "partial0 = partial_cost_theta0(1, 1, pga.distance, pga.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## putting together everything: Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x is our feature vector -- distance\n",
    "# y is our target variable -- accuracy\n",
    "# alpha is the learning rate\n",
    "# theta0 is the intial theta0 \n",
    "# theta1 is the intial theta1\n",
    "def gradient_descent(x, y, alpha=0.1, theta0=0, theta1=0):\n",
    "    max_epochs = 1000 # Maximum number of iterations\n",
    "    counter = 0      # Intialize a counter\n",
    "    c = cost(theta1, theta0, pga.distance, pga.accuracy)  ## Initial cost\n",
    "    costs = [c]     # Lets store each update\n",
    "    # Set a convergence threshold to find where the cost function in minimized\n",
    "    # When the difference between the previous cost and current cost \n",
    "    #        is less than this value we will say the parameters converged\n",
    "    convergence_thres = 0.000001  \n",
    "    cprev = c + 10   \n",
    "    theta0s = [theta0]\n",
    "    theta1s = [theta1]\n",
    "\n",
    "    # When the costs converge or we hit a large number of iterations will we stop updating\n",
    "    while (np.abs(cprev - c) > convergence_thres) and (counter < max_epochs):\n",
    "        cprev = c\n",
    "        # Alpha times the partial deriviative is our updated\n",
    "        update0 = alpha * partial_cost_theta0(theta0, theta1, x, y)\n",
    "        update1 = alpha * partial_cost_theta1(theta0, theta1, x, y)\n",
    "\n",
    "        # Update theta0 and theta1 at the same time\n",
    "        # We want to compute the slopes at the same set of hypothesised parameters\n",
    "        #             so we update after finding the partial derivatives\n",
    "        theta0 -= update0\n",
    "        theta1 -= update1\n",
    "        \n",
    "        # Store thetas\n",
    "        theta0s.append(theta0)\n",
    "        theta1s.append(theta1)\n",
    "        \n",
    "        # Compute the new cost\n",
    "        c = cost(theta0, theta1, pga.distance, pga.accuracy)\n",
    "\n",
    "        # Store updates\n",
    "        costs.append(c)\n",
    "        counter += 1   # Count\n",
    "\n",
    "    return {'theta0': theta0, 'theta1': theta1, \"costs\": costs}\n",
    "\n",
    "print(\"Theta1 =\", gradient_descent(pga.distance, pga.accuracy)['theta1'])\n",
    "descend = gradient_descent(pga.distance, pga.accuracy, alpha=.01)\n",
    "plt.scatter(range(len(descend[\"costs\"])), descend[\"costs\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
